# -*- coding: utf-8 -*-
"""HeartBeat_PipeLine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gBHXSqF1ATpI7xmOlWq5BiBVc7K_HdGE
"""

!kaggle datasets download -d shayanfazeli/heartbeat

import zipfile

# Unzip the dataset
with zipfile.ZipFile('heartbeat.zip', 'r') as zip_ref:
    zip_ref.extractall('dataset_directory')

import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

Train_data = pd.read_csv('dataset_directory/mitbih_train.csv', header=None)
Test_data = pd.read_csv('dataset_directory/mitbih_test.csv', header=None)

Train_data.head()

# Check NULL Values
print(Train_data.isnull().sum().sum())
print(Test_data.isnull().sum().sum())

Train_data.describe()

from sklearn.utils import shuffle

# Shuffle the training data
Train_data = shuffle(Train_data, random_state=42)

# Split the data for training and testing
X_train = Train_data.iloc[:, :100]
y_train = Train_data.iloc[:, -1]

X_test = Test_data.iloc[:, :100]
y_test = Test_data.iloc[:, -1]

# Select all samples from class 0 in the training set
X_train_class_0 = pd.DataFrame(X_train[y_train == 0])
y_train_class_0 = pd.Series(y_train[y_train == 0])

# Select all samples from class 1 in the training set
X_train_class_1 = pd.DataFrame(X_train[y_train == 1])
y_train_class_1 = pd.Series(y_train[y_train == 1])

# Select all samples from class 2 in the training set
X_train_class_2 = pd.DataFrame(X_train[y_train == 2])
y_train_class_2 = pd.Series(y_train[y_train == 2])

# Select all samples from class 3 in the training set
X_train_class_3 = pd.DataFrame(X_train[y_train == 3])
y_train_class_3 = pd.Series(y_train[y_train == 3])

# Select all samples from class 4 in the training set
X_train_class_4 = pd.DataFrame(X_train[y_train == 4])
y_train_class_4 = pd.Series(y_train[y_train == 4])

# Select all samples from class 0 in the test set
X_test_class_0 = pd.DataFrame(X_test[y_test == 0])
y_test_class_0 = pd.Series(y_test[y_test == 0])

# Select all samples from class 1 in the test set
X_test_class_1 = pd.DataFrame(X_test[y_test == 1])
y_test_class_1 = pd.Series(y_test[y_test == 1])

# Select all samples from class 2 in the test set
X_test_class_2 = pd.DataFrame(X_test[y_test == 2])
y_test_class_2 = pd.Series(y_test[y_test == 2])

# Select all samples from class 3 in the test set
X_test_class_3 = pd.DataFrame(X_test[y_test == 3])
y_test_class_3 = pd.Series(y_test[y_test == 3])

# Select all samples from class 4 in the test set
X_test_class_4 = pd.DataFrame(X_test[y_test == 4])
y_test_class_4 = pd.Series(y_test[y_test == 4])

# Combine class 0 with the rest (1, 2, 3, 4) for the training set
X_train_other = pd.concat([X_train_class_1, X_train_class_2, X_train_class_3, X_train_class_4], ignore_index=True)
y_train_other = pd.concat([y_train_class_1, y_train_class_2, y_train_class_3, y_train_class_4], ignore_index=True)

# Combine class 0 with the rest for the training set
X_train_combined = pd.concat([X_train_class_0, X_train_other], ignore_index=True)
y_train_combined = pd.concat([y_train_class_0, y_train_other], ignore_index=True)

# Update y_train labels: class 0 remains 0, all others become 1
y_train_combined = (y_train_combined != 0).astype(int)

# Combine class 0 with the rest (1, 2, 3, 4) for the test set
X_test_other = pd.concat([X_test_class_1, X_test_class_2, X_test_class_3, X_test_class_4], ignore_index=True)
y_test_other = pd.concat([y_test_class_1, y_test_class_2, y_test_class_3, y_test_class_4], ignore_index=True)

# Combine class 0 with the rest for the test set
X_test_combined = pd.concat([X_test_class_0, X_test_other], ignore_index=True)
y_test_combined = pd.concat([y_test_class_0, y_test_other], ignore_index=True)

# Update y_test labels: class 0 remains 0, all others become 1
y_test_combined = (y_test_combined != 0).astype(int)

# Print the shape of the combined test set samples
print(X_test_combined.shape)
print(y_test_combined.shape)

import xgboost as xgb
from imblearn.over_sampling import SMOTE

# Apply SMOTE to handle imbalance for X_train_combined and y_train_combined
smote_combined = SMOTE(
    sampling_strategy=0.5,   # Make minority class 50% of the majority class
    k_neighbors=7,           # Use 7 nearest neighbors
    random_state=42          # Set random state for reproducibility
)
X_train_smote_combined, y_train_smote_combined = smote_combined.fit_resample(X_train_combined, y_train_combined)

# Create the XGBoost model for combined classes using SMOTE-balanced data
xgb_combined = xgb.XGBClassifier(
    scale_pos_weight=(18118 / 3774),  # Adjust for imbalance if needed
    eval_metric='logloss'
)

#train the XGBoost model for combined classes using SMOTE-balanced data
xgb_combined.fit(X_train_smote_combined, y_train_smote_combined)

# Predict on the test set for combined classes
y_pred_combined = xgb_combined.predict(X_test_combined)

joblib.dump(xgb_combined, 'Binary classifier.pkl')

# Evaluate the model for combined classes
accuracy_combined = accuracy_score(y_test_combined, y_pred_combined)
classification_report_combined = classification_report(y_test_combined, y_pred_combined)

print("Classification Report for combined classes:\n", classification_report_combined)

# Apply SMOTE to handle imbalance for X_train_other and y_train_other
smote_other = SMOTE(random_state=5)
X_train_smote_other, y_train_smote_other = smote_other.fit_resample(X_train_other, y_train_other - 1)

# Create  the XGBoost model for other classes
xgb_other = xgb.XGBClassifier(
    objective='multi:softmax',
    num_class=4,
    eval_metric='mlogloss',
    learning_rate=0.1,
    n_estimators=100,
    max_depth=6,
    subsample=0.8,
    colsample_bytree=0.8
)

#train the XGBoost model for other classes
xgb_other.fit(X_train_smote_other, y_train_smote_other)

# Predict on the test set for other classes
y_pred_otherr = xgb_other.predict(X_test_other) + 1

joblib.dump(xgb_other, 'sub-classifier.pkl')

# Evaluate the model for other classes
accuracy_other = accuracy_score(y_test_other, y_pred_otherr)
classification_report_other = classification_report(y_test_other, y_pred_otherr)

print("Classification Report for other classes:\n", classification_report_other)

# Final predictions combining both models
y_pred_final = []
for i in range(len(X_test_combined)):
    y_pred_combined = xgb_combined.predict(np.array(X_test_combined.iloc[i]).reshape(1, -1))
    if y_pred_combined == 0:
        y_pred_final.append(0)
    else:
        y_pred_other = xgb_other.predict(np.array(X_test_combined.iloc[i]).reshape(1, -1))
        y_pred_final.append(y_pred_other.item() + 1)

# Evaluate the final model
accuracy_final = accuracy_score(y_test, y_pred_final)
classification_report_final = classification_report(y_test, y_pred_final)

print("Classification Report for the final model:\n", classification_report_final)